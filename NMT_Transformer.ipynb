{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Session12_END2 Seq2seq_Transformer.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnarevi/TSAI_END2.0_Session12/blob/main/NMT_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bIOLeIAkgip"
      },
      "source": [
        "## Preparing Data\n",
        "First, we'll import all the modules as before, with the addition of the matplotlib modules used for viewing the attention.\n",
        "\n",
        "We will be using Multi30k <http://www.statmt.org/wmt16/multimodal-task.html#task1>__ dataset to train a German to English translation model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOba7gKZbWXc",
        "outputId": "64dad7b9-8e14-43ce-d892-c4d741f14761"
      },
      "source": [
        "%%bash\n",
        "pip install -U spacy --quiet\n",
        "python -m spacy download en --quiet\n",
        "python -m spacy download de --quiet"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
            "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-07-31 12:19:47.985799: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-31 12:19:54.515302: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d27S03zbAAPu"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import Multi30k\n",
        "from typing import Iterable, List\n",
        "\n",
        "# from typing import Tuple\n",
        "# from torch import Tensor\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import spacy\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qHi040bkqP7"
      },
      "source": [
        "Next, we'll set the random seed for reproducability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSQEcla0Ad0E"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtBiIlRplNsD"
      },
      "source": [
        "\n",
        "## Data Sourcing and Processing\n",
        "`torchtext` library has utilities for creating datasets that can be easily iterated through for the purposes of creating a language translation model. In this example, we show how to use torchtext's inbuilt datasets, tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor. We will use Multi30k dataset from torchtext library [here](https://pytorch.org/text/stable/datasets.html#multi30k) that yields a pair of source-target raw sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKK9oA7OArZK"
      },
      "source": [
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPubajj7A0pY"
      },
      "source": [
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language=\"de_core_news_sm\")\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language=\"en_core_web_sm\")"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztK5PjShBN_M"
      },
      "source": [
        "# helper function to yield list of tokens\n",
        "\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BeMBFg1BtWG"
      },
      "source": [
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3cCTrkoB7fu"
      },
      "source": [
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  # Training data Iterator \n",
        "  train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "  # Create torchtext's Vocab object \n",
        "  vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN6IJggTCBH6"
      },
      "source": [
        "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
        "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY3qVJbpK_2L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5690ce5e-b47c-4eaa-cc9c-93f27f085a1f"
      },
      "source": [
        "vocab_transform[TGT_LANGUAGE](['I', 'wish', 'to', 'view', 'output'])"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1167, 0, 19, 459, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy0xQVgdLBkm"
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), \n",
        "                      torch.tensor(token_ids), \n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# src and tgt language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    \n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "\n",
        "    return src_batch,tgt_batch"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SLtlOq3mWzi"
      },
      "source": [
        "\n",
        "Create test data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I_59ly4LC4T"
      },
      "source": [
        "test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaBY38nOmb_L"
      },
      "source": [
        "\n",
        "Let's view one example from test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0GNSQSCLEOB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9297f0a-aec0-4f0e-f589-b2e6677b10c7"
      },
      "source": [
        "next(iter(test_iter))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.\\n',\n",
              " 'A man in an orange hat starring at something.\\n')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7ZJLbiRd0TD"
      },
      "source": [
        "SelfAttention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76giYcWxdyDv"
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (self.head_dim * heads == embed_size), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        # Get number of training examples\n",
        "        N = query.shape[0]\n",
        "\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        # Split the embedding into self.heads different pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
        "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
        "        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
        "\n",
        "        # Einsum does matrix mult. for query*keys for each training example\n",
        "        # with every other training example, don't be confused by einsum\n",
        "        # it's just how I like doing matrix multiplication & bmm\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "        # queries shape: (N, query_len, heads, heads_dim),\n",
        "        # keys shape: (N, key_len, heads, heads_dim)\n",
        "        # energy: (N, heads, query_len, key_len)\n",
        "\n",
        "        # Mask padded indices so their weights become 0\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        # Normalize energy values similarly to seq2seq + attention\n",
        "        # so that they sum to 1. Also divide by scaling factor for\n",
        "        # better stability\n",
        "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.heads * self.head_dim\n",
        "        )\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "        # values shape: (N, value_len, heads, heads_dim)\n",
        "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
        "        # we reshape and flatten the last two dimensions.\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        # Linear layer doesn't modify the shape, final shape will be\n",
        "        # (N, query_len, embed_size)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCUiDOE9K6Ok"
      },
      "source": [
        "TransformerBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rHHaQjCK7Km"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "\n",
        "        # Add skip connection, run through normalization and finally dropout\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUt7LCh4K-3K"
      },
      "source": [
        " Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3e6E64BK_Wr"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        device,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_length,\n",
        "    ):\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(\n",
        "                    embed_size,\n",
        "                    heads,\n",
        "                    dropout=dropout,\n",
        "                    forward_expansion=forward_expansion,\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "        out = self.dropout(\n",
        "            (self.word_embedding(x) + self.position_embedding(positions))\n",
        "        )\n",
        "\n",
        "        # In the Encoder the query, key, value are all the same, it's in the\n",
        "        # decoder this will change. This might look a bit odd in this case.\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, mask)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e6RRZcZLDXj"
      },
      "source": [
        "DecoderBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tNHIxVELD0j"
      },
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.attention = SelfAttention(embed_size, heads=heads)\n",
        "        self.transformer_block = TransformerBlock(\n",
        "            embed_size, heads, dropout, forward_expansion\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, value, key, src_mask, trg_mask):\n",
        "        attention = self.attention(x, x, x, trg_mask)\n",
        "        query = self.dropout(self.norm(attention + x))\n",
        "        out = self.transformer_block(value, key, query, src_mask)\n",
        "        return out"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOUUtCY0LKu7"
      },
      "source": [
        "Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5flwZ_BLLFa"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        trg_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        device,\n",
        "        max_length,\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_out = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # x = layer(x, enc_out, src_mask, trg_mask)\n",
        "             x =layer(x, enc_out, enc_out, src_mask,  trg_mask)\n",
        "\n",
        "        out = self.fc_out(x)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hq6HU-6LOkz"
      },
      "source": [
        "Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTvnpAEILPBU"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        trg_pad_idx,\n",
        "        device ,\n",
        "        embed_size=512,\n",
        "        num_layers=6,\n",
        "        forward_expansion=4,\n",
        "        heads=8,\n",
        "        dropout=0,\n",
        "\n",
        "        max_length=100,\n",
        "    ):\n",
        "\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            src_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            device,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            trg_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            device,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        # (N, 1, 1, src_len)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        N, trg_len = trg.shape\n",
        "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
        "            N, 1, trg_len, trg_len\n",
        "        )\n",
        "\n",
        "        return trg_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
        "        return out\n"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPR4_kh6eYav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed88140-aab5-4954-e3f7-5e28191bf83e"
      },
      "source": [
        "# Training hyperparameters\n",
        "# num_epochs = 100\n",
        "# learning_rate = 3e-4\n",
        "# batch_size = 32\n",
        "\n",
        "# Model hyperparameters\n",
        "src_vocab_size = len(vocab_transform[SRC_LANGUAGE])\n",
        "trg_vocab_size = len(vocab_transform[TGT_LANGUAGE])\n",
        "src_pad_idx = PAD_IDX\n",
        "trg_pad_idx = PAD_IDX\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "model = Transformer(src_vocab_size,trg_vocab_size,src_pad_idx,trg_pad_idx,device =device).to(device)\n",
        "# model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(device)\n",
        "src_vocab_size,trg_vocab_size,src_pad_idx,trg_pad_idx"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19215, 10838, 1, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBhX5dKuLNar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c32e0bf-c1c7-4614-a31f-708890f91c61"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (word_embedding): Embedding(19215, 512)\n",
              "    (position_embedding): Embedding(100, 512)\n",
              "    (layers): ModuleList(\n",
              "      (0): TransformerBlock(\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (1): TransformerBlock(\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (2): TransformerBlock(\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (3): TransformerBlock(\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (4): TransformerBlock(\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (5): TransformerBlock(\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (word_embedding): Embedding(10838, 512)\n",
              "    (position_embedding): Embedding(100, 512)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderBlock(\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (transformer_block): TransformerBlock(\n",
              "          (attention): SelfAttention(\n",
              "            (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (1): DecoderBlock(\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (transformer_block): TransformerBlock(\n",
              "          (attention): SelfAttention(\n",
              "            (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (2): DecoderBlock(\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (transformer_block): TransformerBlock(\n",
              "          (attention): SelfAttention(\n",
              "            (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (3): DecoderBlock(\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (transformer_block): TransformerBlock(\n",
              "          (attention): SelfAttention(\n",
              "            (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (4): DecoderBlock(\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (transformer_block): TransformerBlock(\n",
              "          (attention): SelfAttention(\n",
              "            (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (5): DecoderBlock(\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (transformer_block): TransformerBlock(\n",
              "          (attention): SelfAttention(\n",
              "            (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=512, out_features=10838, bias=True)\n",
              "    (dropout): Dropout(p=0, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSdevJJNLPOt",
        "outputId": "e0605a78-1d97-4537-d137-73f44b17ba50"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 51,225,686 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-CvhZwYLQoT"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr = 2e-4)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO11j3WELR5P"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWdCIQjLngvT"
      },
      "source": [
        "\n",
        "Next, we'll define our training and evaluation loops."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CqI6z4wnwbh"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "    \n",
        "    for src, trg in train_dataloader:\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        optimizer.zero_grad() \n",
        "            \n",
        "        output = model(src, trg[:,:-1])#get rid of EOS token\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        clip = 1\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(train_dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src,trg in val_dataloader:\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "\n",
        "        output = model(src, trg[:,:-1])#get rid of EOS token\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(val_dataloader)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp6Ws-7PoeEi",
        "outputId": "d69e4bbf-9de1-44b2-ef86-7545020c5bcf"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 25\n",
        "\n",
        "train_los=[]\n",
        "val_los=[]\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(model, optimizer)\n",
        "    train_los.append(train_loss)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(model)\n",
        "    val_los.append(val_loss)\n",
        "\n",
        "        \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'saved-model.pt')\n",
        "    # print(f'\\tEpoch: {epoch}')\n",
        "    print(f'\\t Epoch: {epoch}  | Epoch time : {(end_time - start_time):.3f}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. PPL: {math.exp(val_loss):7.3f}')\n",
        "    # print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f},Train PPL: {math.exp(train_loss):7.3f} \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t Epoch: 1  | Epoch time : 65.129s\n",
            "\tTrain Loss: 6.000 | Train PPL: 403.242\n",
            "\t Val. Loss: 5.435 |  Val. PPL: 229.355\n",
            "\t Epoch: 2  | Epoch time : 64.965s\n",
            "\tTrain Loss: 5.405 | Train PPL: 222.542\n",
            "\t Val. Loss: 5.435 |  Val. PPL: 229.311\n",
            "\t Epoch: 3  | Epoch time : 64.865s\n",
            "\tTrain Loss: 5.392 | Train PPL: 219.732\n",
            "\t Val. Loss: 5.451 |  Val. PPL: 232.967\n",
            "\t Epoch: 4  | Epoch time : 64.851s\n",
            "\tTrain Loss: 5.384 | Train PPL: 217.816\n",
            "\t Val. Loss: 5.466 |  Val. PPL: 236.625\n",
            "\t Epoch: 5  | Epoch time : 64.938s\n",
            "\tTrain Loss: 5.374 | Train PPL: 215.680\n",
            "\t Val. Loss: 5.465 |  Val. PPL: 236.279\n",
            "\t Epoch: 6  | Epoch time : 64.851s\n",
            "\tTrain Loss: 5.353 | Train PPL: 211.175\n",
            "\t Val. Loss: 5.467 |  Val. PPL: 236.776\n",
            "\t Epoch: 7  | Epoch time : 65.303s\n",
            "\tTrain Loss: 5.344 | Train PPL: 209.391\n",
            "\t Val. Loss: 5.474 |  Val. PPL: 238.444\n",
            "\t Epoch: 8  | Epoch time : 64.825s\n",
            "\tTrain Loss: 5.336 | Train PPL: 207.745\n",
            "\t Val. Loss: 5.488 |  Val. PPL: 241.764\n",
            "\t Epoch: 9  | Epoch time : 64.886s\n",
            "\tTrain Loss: 5.329 | Train PPL: 206.229\n",
            "\t Val. Loss: 5.494 |  Val. PPL: 243.191\n",
            "\t Epoch: 10  | Epoch time : 64.915s\n",
            "\tTrain Loss: 5.322 | Train PPL: 204.831\n",
            "\t Val. Loss: 5.502 |  Val. PPL: 245.228\n",
            "\t Epoch: 11  | Epoch time : 64.888s\n",
            "\tTrain Loss: 5.316 | Train PPL: 203.544\n",
            "\t Val. Loss: 5.514 |  Val. PPL: 248.127\n",
            "\t Epoch: 12  | Epoch time : 64.905s\n",
            "\tTrain Loss: 5.310 | Train PPL: 202.313\n",
            "\t Val. Loss: 5.520 |  Val. PPL: 249.521\n",
            "\t Epoch: 13  | Epoch time : 64.852s\n",
            "\tTrain Loss: 5.305 | Train PPL: 201.266\n",
            "\t Val. Loss: 5.528 |  Val. PPL: 251.544\n",
            "\t Epoch: 14  | Epoch time : 64.975s\n",
            "\tTrain Loss: 5.300 | Train PPL: 200.429\n",
            "\t Val. Loss: 5.541 |  Val. PPL: 254.850\n",
            "\t Epoch: 15  | Epoch time : 66.147s\n",
            "\tTrain Loss: 4.868 | Train PPL: 130.007\n",
            "\t Val. Loss: 4.602 |  Val. PPL:  99.710\n",
            "\t Epoch: 16  | Epoch time : 66.816s\n",
            "\tTrain Loss: 4.298 | Train PPL:  73.529\n",
            "\t Val. Loss: 4.359 |  Val. PPL:  78.200\n",
            "\t Epoch: 17  | Epoch time : 67.239s\n",
            "\tTrain Loss: 4.108 | Train PPL:  60.823\n",
            "\t Val. Loss: 4.260 |  Val. PPL:  70.791\n",
            "\t Epoch: 18  | Epoch time : 66.810s\n",
            "\tTrain Loss: 4.009 | Train PPL:  55.113\n",
            "\t Val. Loss: 4.210 |  Val. PPL:  67.372\n",
            "\t Epoch: 19  | Epoch time : 66.849s\n",
            "\tTrain Loss: 3.932 | Train PPL:  51.027\n",
            "\t Val. Loss: 4.180 |  Val. PPL:  65.350\n",
            "\t Epoch: 20  | Epoch time : 66.806s\n",
            "\tTrain Loss: 3.868 | Train PPL:  47.848\n",
            "\t Val. Loss: 4.156 |  Val. PPL:  63.819\n",
            "\t Epoch: 21  | Epoch time : 66.748s\n",
            "\tTrain Loss: 3.811 | Train PPL:  45.182\n",
            "\t Val. Loss: 4.132 |  Val. PPL:  62.314\n",
            "\t Epoch: 22  | Epoch time : 67.077s\n",
            "\tTrain Loss: 3.762 | Train PPL:  43.026\n",
            "\t Val. Loss: 4.130 |  Val. PPL:  62.192\n",
            "\t Epoch: 23  | Epoch time : 66.798s\n",
            "\tTrain Loss: 3.718 | Train PPL:  41.171\n",
            "\t Val. Loss: 4.122 |  Val. PPL:  61.695\n",
            "\t Epoch: 24  | Epoch time : 66.912s\n",
            "\tTrain Loss: 3.680 | Train PPL:  39.661\n",
            "\t Val. Loss: 4.083 |  Val. PPL:  59.310\n",
            "\t Epoch: 25  | Epoch time : 66.854s\n",
            "\tTrain Loss: 3.640 | Train PPL:  38.099\n",
            "\t Val. Loss: 4.097 |  Val. PPL:  60.181\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYbkIRrEvVCX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "f153df0e-f99b-4a0e-88f3-4e288502fc63"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-darkgrid')\n",
        "plt.figure()\n",
        "plt.plot(train_los, color = 'magenta')\n",
        "plt.plot(val_los, color = '#606060')\n",
        "plt.title('Train and test Loss')\n",
        "plt.legend(['train_loss', 'validation_loss'], loc = 'upper right')\n",
        "plt.grid(axis = 'y', c = 'black', alpha = 0.2)\n",
        "plt.grid(axis = 'x', c = 'black', alpha = 0.2)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}