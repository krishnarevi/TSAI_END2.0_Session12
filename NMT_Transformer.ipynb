{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Session12_END2 Seq2seq_Transformer.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnarevi/TSAI_END2.0_Session12/blob/main/NMT_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bIOLeIAkgip"
      },
      "source": [
        "## Preparing Data\n",
        "First, we'll import all the modules as before, with the addition of the matplotlib modules used for viewing the attention.\n",
        "\n",
        "We will be using Multi30k <http://www.statmt.org/wmt16/multimodal-task.html#task1>__ dataset to train a German to English translation model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOba7gKZbWXc",
        "outputId": "d2839892-1e7f-4fe9-ea21-3bd6e655e385"
      },
      "source": [
        "%%bash\n",
        "pip install -U spacy --quiet\n",
        "python -m spacy download en --quiet\n",
        "python -m spacy download de --quiet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
            "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-07-31 14:39:07.909147: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-31 14:39:14.281837: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d27S03zbAAPu"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import Multi30k\n",
        "from typing import Iterable, List\n",
        "\n",
        "# from typing import Tuple\n",
        "# from torch import Tensor\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import spacy\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qHi040bkqP7"
      },
      "source": [
        "Next, we'll set the random seed for reproducability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSQEcla0Ad0E"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtBiIlRplNsD"
      },
      "source": [
        "\n",
        "## Data Sourcing and Processing\n",
        "`torchtext` library has utilities for creating datasets that can be easily iterated through for the purposes of creating a language translation model. In this example, we show how to use torchtext's inbuilt datasets, tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor. We will use Multi30k dataset from torchtext library [here](https://pytorch.org/text/stable/datasets.html#multi30k) that yields a pair of source-target raw sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKK9oA7OArZK"
      },
      "source": [
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPubajj7A0pY"
      },
      "source": [
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language=\"de_core_news_sm\")\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language=\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztK5PjShBN_M"
      },
      "source": [
        "# helper function to yield list of tokens\n",
        "\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BeMBFg1BtWG"
      },
      "source": [
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3cCTrkoB7fu"
      },
      "source": [
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  # Training data Iterator \n",
        "  train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "  # Create torchtext's Vocab object \n",
        "  vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN6IJggTCBH6"
      },
      "source": [
        "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
        "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY3qVJbpK_2L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18929792-4ea8-4ca8-b133-91dfe43ce388"
      },
      "source": [
        "vocab_transform[TGT_LANGUAGE](['I', 'wish', 'to', 'view', 'output'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1167, 0, 19, 459, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy0xQVgdLBkm"
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), \n",
        "                      torch.tensor(token_ids), \n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# src and tgt language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    \n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "\n",
        "    return src_batch,tgt_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SLtlOq3mWzi"
      },
      "source": [
        "\n",
        "Create test data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I_59ly4LC4T"
      },
      "source": [
        "test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaBY38nOmb_L"
      },
      "source": [
        "\n",
        "Let's view one example from test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0GNSQSCLEOB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "072ce567-c5fc-41f1-d3e2-09bcfeea731e"
      },
      "source": [
        "next(iter(test_iter))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.\\n',\n",
              " 'A man in an orange hat starring at something.\\n')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7ZJLbiRd0TD"
      },
      "source": [
        "SelfAttention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76giYcWxdyDv"
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (self.head_dim * heads == embed_size), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        # Get number of training examples\n",
        "        N = query.shape[0]\n",
        "\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        # Split the embedding into self.heads different pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
        "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
        "        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
        "\n",
        "        # Einsum does matrix mult. for query*keys for each training example\n",
        "        # with every other training example, don't be confused by einsum\n",
        "        # it's just how I like doing matrix multiplication & bmm\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "        # queries shape: (N, query_len, heads, heads_dim),\n",
        "        # keys shape: (N, key_len, heads, heads_dim)\n",
        "        # energy: (N, heads, query_len, key_len)\n",
        "\n",
        "        # Mask padded indices so their weights become 0\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        # Normalize energy values similarly to seq2seq + attention\n",
        "        # so that they sum to 1. Also divide by scaling factor for\n",
        "        # better stability\n",
        "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.heads * self.head_dim\n",
        "        )\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "        # values shape: (N, value_len, heads, heads_dim)\n",
        "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
        "        # we reshape and flatten the last two dimensions.\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        # Linear layer doesn't modify the shape, final shape will be\n",
        "        # (N, query_len, embed_size)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCUiDOE9K6Ok"
      },
      "source": [
        "TransformerBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rHHaQjCK7Km"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "\n",
        "        # Add skip connection, run through normalization and finally dropout\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUt7LCh4K-3K"
      },
      "source": [
        " Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3e6E64BK_Wr"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        device,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_length,\n",
        "    ):\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(\n",
        "                    embed_size,\n",
        "                    heads,\n",
        "                    dropout=dropout,\n",
        "                    forward_expansion=forward_expansion,\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "        out = self.dropout(\n",
        "            (self.word_embedding(x) + self.position_embedding(positions))\n",
        "        )\n",
        "\n",
        "        # In the Encoder the query, key, value are all the same, it's in the\n",
        "        # decoder this will change. This might look a bit odd in this case.\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, mask)\n",
        "\n",
        "        return out\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        \n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e6RRZcZLDXj"
      },
      "source": [
        "DecoderBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tNHIxVELD0j"
      },
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.attention = SelfAttention(embed_size, heads=heads)\n",
        "        self.transformer_block = TransformerBlock(\n",
        "            embed_size, heads, dropout, forward_expansion\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, value, key, src_mask, trg_mask):\n",
        "        attention = self.attention(x, x, x, trg_mask)\n",
        "        query = self.dropout(self.norm(attention + x))\n",
        "        out = self.transformer_block(value, key, query, src_mask)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOUUtCY0LKu7"
      },
      "source": [
        "Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5flwZ_BLLFa"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        trg_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        device,\n",
        "        max_length,\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_out = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # x = layer(x, enc_out, src_mask, trg_mask)\n",
        "             x =layer(x, enc_out, enc_out, src_mask,  trg_mask)\n",
        "\n",
        "        out = self.fc_out(x)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hq6HU-6LOkz"
      },
      "source": [
        "Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTvnpAEILPBU"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        trg_pad_idx,\n",
        "        device ,\n",
        "        embed_size=512,\n",
        "        num_layers=6,\n",
        "        forward_expansion=4,\n",
        "        heads=8,\n",
        "        dropout=0,\n",
        "\n",
        "        max_length=100,\n",
        "    ):\n",
        "\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            src_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            device,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            trg_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            device,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        # (N, 1, 1, src_len)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        N, trg_len = trg.shape\n",
        "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
        "            N, 1, trg_len, trg_len\n",
        "        )\n",
        "\n",
        "        return trg_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPR4_kh6eYav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5feca30c-ae59-4bea-f48c-586a8e63acad"
      },
      "source": [
        "# Training hyperparameters\n",
        "# num_epochs = 100\n",
        "# learning_rate = 3e-4\n",
        "# batch_size = 32\n",
        "\n",
        "# Model hyperparameters\n",
        "src_vocab_size = len(vocab_transform[SRC_LANGUAGE])\n",
        "trg_vocab_size = len(vocab_transform[TGT_LANGUAGE])\n",
        "src_pad_idx = PAD_IDX\n",
        "trg_pad_idx = PAD_IDX\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "model = Transformer(src_vocab_size,trg_vocab_size,src_pad_idx,trg_pad_idx,device =device).to(device)\n",
        "# model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(device)\n",
        "src_vocab_size,trg_vocab_size,src_pad_idx,trg_pad_idx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19215, 10838, 1, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBhX5dKuLNar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aaf214f-efea-4324-a7a7-454c6aae4761"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (word_embedding): Embedding(19215, 512)\n",
              "    (position_embedding): Embedding(100, 512)\n",
              "    (layers): ModuleList(\n",
              "      (0): TransformerBlock(\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (1): TransformerBlock(\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (2): TransformerBlock(\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (3): TransformerBlock(\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (4): TransformerBlock(\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (5): TransformerBlock(\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (word_embedding): Embedding(10838, 512)\n",
              "    (position_embedding): Embedding(100, 512)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderBlock(\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (transformer_block): TransformerBlock(\n",
              "          (attention): SelfAttention(\n",
              "            (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (1): DecoderBlock(\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (transformer_block): TransformerBlock(\n",
              "          (attention): SelfAttention(\n",
              "            (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (2): DecoderBlock(\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (transformer_block): TransformerBlock(\n",
              "          (attention): SelfAttention(\n",
              "            (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (3): DecoderBlock(\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (transformer_block): TransformerBlock(\n",
              "          (attention): SelfAttention(\n",
              "            (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (4): DecoderBlock(\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (transformer_block): TransformerBlock(\n",
              "          (attention): SelfAttention(\n",
              "            (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (5): DecoderBlock(\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention): SelfAttention(\n",
              "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (transformer_block): TransformerBlock(\n",
              "          (attention): SelfAttention(\n",
              "            (values): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (keys): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (queries): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=512, out_features=10838, bias=True)\n",
              "    (dropout): Dropout(p=0, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSdevJJNLPOt",
        "outputId": "f857b11a-fc85-4252-cc4d-49bbf7ed381e"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 51,225,686 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-CvhZwYLQoT"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr = 2e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO11j3WELR5P"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWdCIQjLngvT"
      },
      "source": [
        "\n",
        "Next, we'll define our training and evaluation loops."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CqI6z4wnwbh"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "    \n",
        "    for src, trg in train_dataloader:\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        optimizer.zero_grad() \n",
        "            \n",
        "        output = model(src, trg[:,:-1])#get rid of EOS token\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        clip = 1\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(train_dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src,trg in val_dataloader:\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "\n",
        "        output = model(src, trg[:,:-1])#get rid of EOS token\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(val_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp6Ws-7PoeEi",
        "outputId": "2f302b19-dc89-42bd-850a-a8782ce1cad1"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "\n",
        "train_los=[]\n",
        "val_los=[]\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(model, optimizer)\n",
        "    train_los.append(train_loss)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(model)\n",
        "    val_los.append(val_loss)\n",
        "\n",
        "        \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'saved-model.pt')\n",
        "    # print(f'\\tEpoch: {epoch}')\n",
        "    print(f'\\t Epoch: {epoch}  | Epoch time : {(end_time - start_time):.3f}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. PPL: {math.exp(val_loss):7.3f}')\n",
        "    # print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f},Train PPL: {math.exp(train_loss):7.3f} \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t Epoch: 1  | Epoch time : 64.714s\n",
            "\tTrain Loss: 6.000 | Train PPL: 403.242\n",
            "\t Val. Loss: 5.435 |  Val. PPL: 229.355\n",
            "\t Epoch: 2  | Epoch time : 64.364s\n",
            "\tTrain Loss: 5.405 | Train PPL: 222.542\n",
            "\t Val. Loss: 5.435 |  Val. PPL: 229.311\n",
            "\t Epoch: 3  | Epoch time : 64.413s\n",
            "\tTrain Loss: 5.392 | Train PPL: 219.732\n",
            "\t Val. Loss: 5.451 |  Val. PPL: 232.967\n",
            "\t Epoch: 4  | Epoch time : 64.414s\n",
            "\tTrain Loss: 5.384 | Train PPL: 217.816\n",
            "\t Val. Loss: 5.466 |  Val. PPL: 236.625\n",
            "\t Epoch: 5  | Epoch time : 64.707s\n",
            "\tTrain Loss: 5.374 | Train PPL: 215.680\n",
            "\t Val. Loss: 5.465 |  Val. PPL: 236.279\n",
            "\t Epoch: 6  | Epoch time : 64.434s\n",
            "\tTrain Loss: 5.353 | Train PPL: 211.175\n",
            "\t Val. Loss: 5.467 |  Val. PPL: 236.776\n",
            "\t Epoch: 7  | Epoch time : 64.394s\n",
            "\tTrain Loss: 5.344 | Train PPL: 209.391\n",
            "\t Val. Loss: 5.474 |  Val. PPL: 238.444\n",
            "\t Epoch: 8  | Epoch time : 64.379s\n",
            "\tTrain Loss: 5.336 | Train PPL: 207.745\n",
            "\t Val. Loss: 5.488 |  Val. PPL: 241.764\n",
            "\t Epoch: 9  | Epoch time : 64.658s\n",
            "\tTrain Loss: 5.329 | Train PPL: 206.229\n",
            "\t Val. Loss: 5.494 |  Val. PPL: 243.191\n",
            "\t Epoch: 10  | Epoch time : 64.447s\n",
            "\tTrain Loss: 5.322 | Train PPL: 204.831\n",
            "\t Val. Loss: 5.502 |  Val. PPL: 245.228\n",
            "\t Epoch: 11  | Epoch time : 64.395s\n",
            "\tTrain Loss: 5.316 | Train PPL: 203.544\n",
            "\t Val. Loss: 5.514 |  Val. PPL: 248.127\n",
            "\t Epoch: 12  | Epoch time : 64.376s\n",
            "\tTrain Loss: 5.310 | Train PPL: 202.313\n",
            "\t Val. Loss: 5.520 |  Val. PPL: 249.521\n",
            "\t Epoch: 13  | Epoch time : 64.371s\n",
            "\tTrain Loss: 5.305 | Train PPL: 201.266\n",
            "\t Val. Loss: 5.528 |  Val. PPL: 251.544\n",
            "\t Epoch: 14  | Epoch time : 64.419s\n",
            "\tTrain Loss: 5.300 | Train PPL: 200.429\n",
            "\t Val. Loss: 5.541 |  Val. PPL: 254.850\n",
            "\t Epoch: 15  | Epoch time : 65.959s\n",
            "\tTrain Loss: 4.868 | Train PPL: 130.007\n",
            "\t Val. Loss: 4.602 |  Val. PPL:  99.710\n",
            "\t Epoch: 16  | Epoch time : 66.021s\n",
            "\tTrain Loss: 4.298 | Train PPL:  73.529\n",
            "\t Val. Loss: 4.359 |  Val. PPL:  78.200\n",
            "\t Epoch: 17  | Epoch time : 66.150s\n",
            "\tTrain Loss: 4.108 | Train PPL:  60.823\n",
            "\t Val. Loss: 4.260 |  Val. PPL:  70.791\n",
            "\t Epoch: 18  | Epoch time : 66.085s\n",
            "\tTrain Loss: 4.009 | Train PPL:  55.113\n",
            "\t Val. Loss: 4.210 |  Val. PPL:  67.372\n",
            "\t Epoch: 19  | Epoch time : 66.330s\n",
            "\tTrain Loss: 3.932 | Train PPL:  51.027\n",
            "\t Val. Loss: 4.180 |  Val. PPL:  65.350\n",
            "\t Epoch: 20  | Epoch time : 66.007s\n",
            "\tTrain Loss: 3.868 | Train PPL:  47.848\n",
            "\t Val. Loss: 4.156 |  Val. PPL:  63.819\n",
            "\t Epoch: 21  | Epoch time : 66.108s\n",
            "\tTrain Loss: 3.811 | Train PPL:  45.182\n",
            "\t Val. Loss: 4.132 |  Val. PPL:  62.314\n",
            "\t Epoch: 22  | Epoch time : 66.017s\n",
            "\tTrain Loss: 3.762 | Train PPL:  43.026\n",
            "\t Val. Loss: 4.130 |  Val. PPL:  62.192\n",
            "\t Epoch: 23  | Epoch time : 66.046s\n",
            "\tTrain Loss: 3.718 | Train PPL:  41.171\n",
            "\t Val. Loss: 4.122 |  Val. PPL:  61.695\n",
            "\t Epoch: 24  | Epoch time : 66.042s\n",
            "\tTrain Loss: 3.680 | Train PPL:  39.661\n",
            "\t Val. Loss: 4.083 |  Val. PPL:  59.310\n",
            "\t Epoch: 25  | Epoch time : 66.235s\n",
            "\tTrain Loss: 3.640 | Train PPL:  38.099\n",
            "\t Val. Loss: 4.097 |  Val. PPL:  60.181\n",
            "\t Epoch: 26  | Epoch time : 66.017s\n",
            "\tTrain Loss: 3.603 | Train PPL:  36.705\n",
            "\t Val. Loss: 4.135 |  Val. PPL:  62.518\n",
            "\t Epoch: 27  | Epoch time : 65.955s\n",
            "\tTrain Loss: 3.565 | Train PPL:  35.347\n",
            "\t Val. Loss: 4.143 |  Val. PPL:  63.012\n",
            "\t Epoch: 28  | Epoch time : 66.027s\n",
            "\tTrain Loss: 3.534 | Train PPL:  34.248\n",
            "\t Val. Loss: 4.109 |  Val. PPL:  60.915\n",
            "\t Epoch: 29  | Epoch time : 66.340s\n",
            "\tTrain Loss: 3.500 | Train PPL:  33.112\n",
            "\t Val. Loss: 4.125 |  Val. PPL:  61.867\n",
            "\t Epoch: 30  | Epoch time : 66.010s\n",
            "\tTrain Loss: 3.467 | Train PPL:  32.040\n",
            "\t Val. Loss: 4.128 |  Val. PPL:  62.048\n",
            "\t Epoch: 31  | Epoch time : 65.930s\n",
            "\tTrain Loss: 3.437 | Train PPL:  31.102\n",
            "\t Val. Loss: 4.152 |  Val. PPL:  63.554\n",
            "\t Epoch: 32  | Epoch time : 65.961s\n",
            "\tTrain Loss: 3.411 | Train PPL:  30.295\n",
            "\t Val. Loss: 4.115 |  Val. PPL:  61.235\n",
            "\t Epoch: 33  | Epoch time : 66.002s\n",
            "\tTrain Loss: 3.379 | Train PPL:  29.352\n",
            "\t Val. Loss: 4.129 |  Val. PPL:  62.134\n",
            "\t Epoch: 34  | Epoch time : 66.084s\n",
            "\tTrain Loss: 3.346 | Train PPL:  28.385\n",
            "\t Val. Loss: 4.123 |  Val. PPL:  61.763\n",
            "\t Epoch: 35  | Epoch time : 65.976s\n",
            "\tTrain Loss: 3.320 | Train PPL:  27.656\n",
            "\t Val. Loss: 4.138 |  Val. PPL:  62.670\n",
            "\t Epoch: 36  | Epoch time : 65.956s\n",
            "\tTrain Loss: 3.282 | Train PPL:  26.625\n",
            "\t Val. Loss: 4.153 |  Val. PPL:  63.595\n",
            "\t Epoch: 37  | Epoch time : 66.015s\n",
            "\tTrain Loss: 3.252 | Train PPL:  25.850\n",
            "\t Val. Loss: 4.166 |  Val. PPL:  64.446\n",
            "\t Epoch: 38  | Epoch time : 66.084s\n",
            "\tTrain Loss: 3.221 | Train PPL:  25.042\n",
            "\t Val. Loss: 4.199 |  Val. PPL:  66.637\n",
            "\t Epoch: 39  | Epoch time : 66.464s\n",
            "\tTrain Loss: 3.191 | Train PPL:  24.303\n",
            "\t Val. Loss: 4.202 |  Val. PPL:  66.832\n",
            "\t Epoch: 40  | Epoch time : 65.864s\n",
            "\tTrain Loss: 3.160 | Train PPL:  23.576\n",
            "\t Val. Loss: 4.209 |  Val. PPL:  67.296\n",
            "\t Epoch: 41  | Epoch time : 65.873s\n",
            "\tTrain Loss: 3.132 | Train PPL:  22.914\n",
            "\t Val. Loss: 4.185 |  Val. PPL:  65.719\n",
            "\t Epoch: 42  | Epoch time : 65.966s\n",
            "\tTrain Loss: 3.104 | Train PPL:  22.277\n",
            "\t Val. Loss: 4.210 |  Val. PPL:  67.333\n",
            "\t Epoch: 43  | Epoch time : 65.986s\n",
            "\tTrain Loss: 3.073 | Train PPL:  21.607\n",
            "\t Val. Loss: 4.224 |  Val. PPL:  68.273\n",
            "\t Epoch: 44  | Epoch time : 65.953s\n",
            "\tTrain Loss: 3.041 | Train PPL:  20.921\n",
            "\t Val. Loss: 4.239 |  Val. PPL:  69.352\n",
            "\t Epoch: 45  | Epoch time : 65.916s\n",
            "\tTrain Loss: 2.995 | Train PPL:  19.990\n",
            "\t Val. Loss: 4.252 |  Val. PPL:  70.261\n",
            "\t Epoch: 46  | Epoch time : 65.935s\n",
            "\tTrain Loss: 2.981 | Train PPL:  19.701\n",
            "\t Val. Loss: 4.257 |  Val. PPL:  70.577\n",
            "\t Epoch: 47  | Epoch time : 65.933s\n",
            "\tTrain Loss: 2.950 | Train PPL:  19.106\n",
            "\t Val. Loss: 4.257 |  Val. PPL:  70.580\n",
            "\t Epoch: 48  | Epoch time : 65.966s\n",
            "\tTrain Loss: 2.912 | Train PPL:  18.397\n",
            "\t Val. Loss: 4.280 |  Val. PPL:  72.252\n",
            "\t Epoch: 49  | Epoch time : 66.419s\n",
            "\tTrain Loss: 2.874 | Train PPL:  17.708\n",
            "\t Val. Loss: 4.295 |  Val. PPL:  73.346\n",
            "\t Epoch: 50  | Epoch time : 65.871s\n",
            "\tTrain Loss: 2.839 | Train PPL:  17.099\n",
            "\t Val. Loss: 4.328 |  Val. PPL:  75.776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYbkIRrEvVCX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "3e62f708-f00f-4e68-9ef9-b34453a2f463"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-darkgrid')\n",
        "plt.figure()\n",
        "plt.plot(train_los, color = 'magenta')\n",
        "plt.plot(val_los, color = '#606060')\n",
        "plt.title('Train and test Loss')\n",
        "plt.legend(['train_loss', 'validation_loss'], loc = 'upper right')\n",
        "plt.grid(axis = 'y', c = 'black', alpha = 0.2)\n",
        "plt.grid(axis = 'x', c = 'black', alpha = 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePiTos2g0r3r"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7ektZfKsc6G"
      },
      "source": [
        "def translate_sentence(sentence, model, device, max_len = 50):\n",
        "    \n",
        "    model.eval()\n",
        "        \n",
        "    src_indexes = text_transform[SRC_LANGUAGE](sentence)\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    # print(src_tensor.shape)\n",
        "    #src = [batch size, src len]\n",
        "    #trg = [batch size, trg len]\n",
        "    \n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "    # print(src_mask.shape)#[1,1,1,13]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "        # print(\"enc_src\",enc_src)\n",
        "\n",
        "    trg_indexes = vocab_transform[TGT_LANGUAGE](['<bos>'])#vocab_transform[TGT_LANGUAGE](['<bos>'])\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "        # print(trg_tensor.shape)\n",
        "\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        # print(trg_mask.shape)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output = model.decoder(trg_tensor, enc_src, src_mask, trg_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == EOS_IDX:\n",
        "            break\n",
        "    \n",
        "    trg_tokens =  vocab_transform[TGT_LANGUAGE].lookup_tokens(list(trg_indexes))\n",
        "    pred_sentence =\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(trg_indexes))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
        "    \n",
        "    return pred_sentence, trg_tokens[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqyHjeUW0vHJ"
      },
      "source": [
        "Now Let's see how our model is performing by predicting few samples from test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCNgise62BLq",
        "outputId": "f15ee82e-e89a-4e8f-c14f-acf8bf27a186"
      },
      "source": [
        "src,trg=next(iter(test_iter))\n",
        "src=src.rstrip(\"\\n\")\n",
        "trg=trg.rstrip(\"\\n\")\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')\n",
        "\n",
        "translation,_ = translate_sentence(src,model,device=device )\n",
        "\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = Eine Blondine hält mit einem Mann im Sand Händchen.\n",
            "trg = A blond holding hands with a guy in the sand.\n",
            "predicted trg =  A man in a black shirt and a woman in a pink shirt and hat . \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3shQ1tUZ2nhj"
      },
      "source": [
        "## BLEU\n",
        "Finally we calculate the BLEU score for the Transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAloObSH2Bqv"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu(data,  model, device = device, max_len = 50):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    # print(\"hi\")\n",
        "    # print(data)\n",
        "    \n",
        "    for datum in data:\n",
        "        # print(datum)\n",
        "        \n",
        "        src = datum[0].rstrip(\"\\n\")\n",
        "        # print(src)\n",
        "        trg = token_transform[TGT_LANGUAGE](datum[1].rstrip(\"\\n\"))\n",
        "        # print(trg)\n",
        "        # print(src)\n",
        "        trg_sent,pred_trg = translate_sentence(src, model, device, max_len)\n",
        "        \n",
        "        #cut off <eos> token\n",
        "        pred_trg = pred_trg[:-1]\n",
        "        \n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append([trg])\n",
        "        print(trg)\n",
        "        print(pred_trg)\n",
        "        # break\n",
        "        \n",
        "    return bleu_score(pred_trgs, trgs,max_n =1,weights=[1.0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXyaT2bV3M63",
        "outputId": "439bd3b2-51af-4646-cdcc-24fc2f13da60"
      },
      "source": [
        "bleu_score = calculate_bleu(test_iter, model)\n",
        "\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Two', 'men', 'pretend', 'to', 'be', 'statutes', 'while', 'women', 'look', 'on', '.']\n",
            "['A', 'man', 'in', 'a', 'black', 'shirt', 'and', 'a', 'woman', 'in', 'a', 'pink', 'shirt', 'and', 'hat', '.']\n",
            "BLEU score = 6.25\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}